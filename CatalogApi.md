# Catalog REST API
This document describes the REST API of the Cerebro Catalog. This is intended for
clients that want to leverage all of the Cerebro functionality. In addition to this,
clients can connect using existing APIs, such as the Hive Metastore API.

NOTE: this should be considered alpha, with the APIs likely to be revised. No compatibility
is guaranteed as this point.

The purpose of this API is to provide programmatic access to interact with the catalog.
For users that want to interactively access the catalog, it is recommended to use the
cerebro_cli, which provides a more traditional CLI experience built ontop of these APIs.

## Dataset

The main object the catalog exposes are datasets. These are all logical and can be
backed by storage systems or derived views. Users should not care about this distinction
and this datasets can be seen as a either a table or view from traditional SQL terminology.
This has already been true from a query point of view (selecting from a table is largely
indistinguishable from selecting from a view) but we will extend this from a data
management perspective as well.

Datasets should not be created to enforce access control but instead to give logical names
to data. Different users reading the same dataset can see different values, depending on
the access policies. This is different than traditional SQL semantics where users might
have to update their request to know which view they are allowed to read due to
permissions.

### API overview
All APIs take JSON as the parameter and if they return anything, returns a JSON object.
This is except the create APIs, which will just return the ID.

### Executing Hive DDL
Endpoint: /api/hive-ddl [POST]
This API allows you to execute HiveQL DDL statements. This can be used instead of the
many of the other APIs if preferred. For example, this can be used to create datasets,
create roles, issues grants, etc. The purpose of this API is to be compatible with
beeline.

The post request can take as parameters:
```
{
    "query" [String]: Required, HiveQL DDL statement.
}
```

### Object definition
```
Dataset {
  "id" [long]: Unique ID generated by Cerebro. This id is never recycled.
  "database" [String]: Optional database this dataset is in.
  "name" [String]: Required, the name of the dataset
  "schema" [list<String>]: List of name:type columns.
}
```
Each dataset has a unique id which is never reused. Each dataset must have a unique
database.name identifier which can be reused if datasets are deleted.

```
Policy {
  "id" [long]: Unique ID generated by Cerebro. This id is never recycled.
  "dataset_id" [Long]: ID of dataset this policy is for.
  "users" [list<String>]: List of users this policy applies to.
  "columns"[list<String>]: Optional, list of granted columns
}
```

### Creating datasets
Creating a dataset is done with a PUT. The result is the dataset id.
Endpoint: /api/datasets PUT
```
{
  "database" [String]: Optional, database for this dataset. Default database
          if unspecified.
  "name"[String]: Required, name of dataset. Combination of database.name must
          be unique.

  "storage_url" [String]: URL of storage for this dataset (e.g. path in S3)
  "schema" [list<String>]: Schema of data. Each element should be <name:type>.

  "base_dataset" [String]: Name of an existing dataset (database.name).

  "projection" [list<String>]: Optional, SQL SELECT clause.
  "filters" [String]: Optional, SQL WHERE clause.

  "hiveQL"[String]: hive ql CREATE TABLE/VIEW statement.
}
```

One of storage_url, base_dataset or hiveQL must be specified. storage_url is used to create
traditional base tables, with the URL being the URL in the underlying storage. The
schema can be optionally provided for cases where the data is not self describing.
base_dataset can be specified if creating views. In either case, projection is a list
of projected columns. This is analogous to the select list from SQL and can contain
functions. For example, "[city, upper(state_code)]" is a valid projection. `filters` is
optional and can be the where clause from SQL. For example "city == SF AND name != NULL".

hiveQL can be specified instead of any of the others and the engine will extract
the relevant parts. This can be any CREATE TABLE/VIEW HQL statement. This is provided
for easy porting of existing DDL workflows. This should be used to specify file format,
partitioning, etc.

### Listing datasets
Endpoint: /api/datasets [GET]
Endpoint: /api/datasets [POST]

The post request can take as parameters:
```
{
  "db" [String]: Optional, database to retrieve datasets from. Default is 'default'.
  "filter" [String]: Optional, filter on the name of datasets to return. For example, 'log*'
      returns all datasets that start with 'log'.
}
```

### Granting policies
Endpoint: /api/grant-policy [PUT]
```
{
  "dataset" [String]: Required, name of dataset to update the policies for.
  "db" [String]: Optional, database this dataset is in. Defaults to 'default'.
  "users" [list<String>]: Required, list of users/groups to update the policies for.

  "projection"[list<String>]: Optional, list of columns to grant to these users.
          If left empty, grants all columns.
}
```
Grants the list of users access to the dataset. Permissions can be controlled by specifying
columns.

### Revoking policies
Endpoint: /api/revoke-policy [PUT]
```
{
  "dataset" [String]: Required, name of dataset to update the policies for.
  "db" [String]: Optional, database this dataset is in. Defaults to 'default'.
  "users" [list<String>]: Required, list of users/groups to update the policies for.
}
```
Revokes access for the list of users for these datasets.

### Viewing the access policies
Endpoint: /api/permissions [POST]

This API returns the access policies by dataset. You can filter on the datasets
you are intersted in as well as the users you are interested in. It takes
```
{
  'users_or_groups' [String]: Optional, comma separated list of users and groups
      to retrieve permissions for. If empty, does so for all groups with any access.
  'db' [String]: Optional, database to query permissions for. Defaults to 'default'.
  'filter' [String]: Optional, dataset filter.
}
```
This returns a list of permissions objects. For each object, it contains the users/groups
that have access and at which level. Each object contains:
```
{
  'database' [String]: Name of database of this object.
  'name' [String]: Name of this object.
  'users/groups': list[String]. The users and groups that have access to this object.
  'roles' list[String]: For each user/group, the role they had which gave them
      access to this object.
  'level' [String]: ALL/READ/WRITE access level.
  'projection' list[String]: The projection within this dataset. If empty, it indicates
      all the fields.
}
```
If a dataset has been configured so that one group of users has access to one set of
columns and another group has access to a different set, this API would return two
permissions objects.

**Getting everything a particular user/group has access to**
This can be done by specifying the user and leaving filter empty.

**Getting all the users that have access to a particular object**
This can be done by specifying the db and filter and leaving the user argument empty.

### Misc functions

##### Deleting datasets
Endpoint: /api/datasets/{name} [DELETE]

Deletes the dataset.

##### Retrieving dataset
Endpoint: /api/datasets/{name} [GET]

Returns: Dataset json.

### Example
Here are a few examples for common tasks using this API.

To register a dataset for every to access, you would:
```
/api/datasets [PUT] { "name": "data", "url": "s3://data" }
/api/grant-policy [PUT] { "dataset": "data", "users" = "admin_group,user1" }
```

To register a dataset where two users have access to different columns:
```
/api/datasets [PUT] { "name": "data", "storage_url": "s3://for-everyone" }
/api/grant-policy [PUT] { "dataset": "data", "users" = ["u1"], "columns" = "c1,c2" }
/api/grant-policy [PUT] { "dataset": "data", "users" = ["u2"], "columns" = "mask(c2)" }
```

### Defining access policies.
We will add 3 policies for 3 different users. The users must already exist on the system. We will
  - Give the Hadoop user just access to the category column.
  - Give the presto user access to just the url column.
  - Give the zeppelin user access to the category column after a transformation is
    applied AND rows are filtered.

```
$ curl -H "Content-Type: application/json" -X PUT -d '{"users":"hadoop","dataset":"products","projection":"category"}' localhost:5000/api/grant-policy
$ curl -H "Content-Type: application/json" -X PUT -d '{"users":"presto","dataset":"products","projection":"url"}' localhost:5000/api/grant-policy
$ curl -H "Content-Type: application/json" -X PUT -d '{"users":"zeppelin","dataset":"products","projection":"upper(category)", "filters":"category < \"m\""}' localhost:5000/api/grant-policy
```

### Reading it.
Again, we can read it with the curl api. Each of these commands should return
different data with the above policies enforced.

```
$ curl <cdas_rest_server_endpoint>/api/scan/products?user=hadoop
$ curl <cdas_rest_server_endpoint>/api/scan/products?user=presto
$ curl <cdas_rest_server_endpoint>/api/scan/products?user=zeppelin
```

Alternatively, the *scanpage* api can be used to return records in batches.  The
api accepts two optional argument _records=_, which limits the number of records
returned in each batch, and _session\_id_.  By default, the api returns 10,000 records.

The _session\_id_ value is used on subsequent queries to return successive batches of
records.  It must be omitted on the first query.
```
$ curl <cdas_rest_server_endpoint>/api/scanpage/products?user=presto
$ curl <cdas_rest_server_endpoint>/api/scanpage/products?user=presto&records=500
$ curl <cdas_rest_server_endpoint>/api/scanpage/products?user=presto&session_id=77480ad07d743bb1:b4f7822f036c6c91
```
Each returned object contains:
```
{
  'records' [List]: Each entry is an object containing the field names, types and values of each record.
  'session_id' [String]: Key used to return subsequent 'pages'.  Each page contains up to 'records' entries.  When the final page is returned, 'session_id' is "-1".
}
```

## Databases

A database is a set of datasets.  Each user has a set of databases that they are authorized
to access.  This set may in practice be empty, which is to say, that user does not have
permissions to access any databases.

### Listing databases
Endpoint: /api/databases?user=<token>

Each returned object contains:
```
{
  'databases' [list<String>]: databases for which the user has permission to access.
}
```
### Example
```
$ curl -H "Content-Type: application/json" -d "{ }" <cdas_rest_server_endpoint>/api/databases?user=presto
```
