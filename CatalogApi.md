# Catalog REST API
This document describes the REST API of the Cerebro Catalog. This is intended for
clients that want to leverage all of the Cerebro functionality. In addition to this,
clients can connect using existing APIs, such as the Hive Metastore API.

NOTE: this should be considered alpha, with the APIs likely to be revised. No compatibility
is guaranteed as this point.

## Dataset

The main object the catalog exposes are datasets. These are all logical and can be
backed by storage systems or derived views. Users should not care about this distinction
and this datasets can be seen as a either a table or view from traditional SQL terminology.
This has already been true from a query point of view (selecting from a table is largely
indistinguishable from selecting from a view) but we will extend this from a data
management perspective as well.

Datasets should not be created to enforce access control but instead to give logical names
to data. Different users reading the same dataset can see different values, depending on
the access policies. This is different than traditional SQL semantics where users might
have to update their request to know which view they are allowed to read due to
permissions.

### API overview
All APIs take JSON as the parameter and if they return anything, returns a JSON object.
This is except the create APIs, which will just return the ID.

### Object definition
```
Dataset {
  "id" [long]: Unique ID generated by Cerebro. This id is never recycled.
  "database" [String]: Optional database this dataset is in.
  "name" [String]: Required, the name of the dataset
  "schema" [list<String>]: List of name:type columns.
}
```
Each dataset has a unique id which is never reused. Each dataset must have a unique
database.name identifier which can be reused if datasets are deleted.

```
Policy {
  "id" [long]: Unique ID generated by Cerebro. This id is never recycled.
  "dataset_id" [Long]: ID of dataset this policy is for.
  "users" [list<String>]: List of users this policy applies to.
  "columns"[list<String>]: Optional, list of granted columns
  "filters"[String] : Optional, WHERE clause of filters.
}
```

### Creating datasets
Creating a dataset is done with a PUT. The result is the dataset id.
Endpoint: /api/datasets PUT
```
{
  "database" [String]: Optional, database for this dataset. Default database
          if unspecified.
  "name"[String]: Required, name of dataset. Combination of database.name must
          be unique.

  "base_url" [String]: URL of storage for this dataset (e.g. path in S3)
  "schema" [list<String>]: Schema of data. Each element should be <name:type>.

  "base_dataset" [String]: Name of an existing dataset (database.name).

  "projection" [list<String>]: Optional, SQL SELECT clause.
  "filters" [String]: Optional, SQL WHERE clause.

  "hiveQL"[String]: hive sql CREATE TABLE/VIEW statement.
}
```

One of base_url, base_dataset or hiveQL must be specified. base_url is used to create
traditional base tables, with the URL being the URL in the underlying storage. The
schema can be optionally provided for cases where the data is not self describing.
base_dataset can be specified if creating views. In either case, projection is a list
of projectedcolumns. This is analogous to the select list from SQL and can contain
functions. For example, "[city, upper(state_code)]" is a valid projection. filters is
optional and can be the where clause from SQL. For example "city == SF AND name != NULL".

hiveQL can be specified instead of any of the others and the engine will extract
the relevant parts. This can be any CREATE TABLE/VIEW HQL statement. This is provided
for easy porting of existing DDL workflows. This should be used to specify file format,
partitioning, etc.

### Granting policies
Endpoing: /api/policies/grant [POST]
```
{
  "dataset-id-or-name" [String]: Required, name or id of dataset to update
          the policies for.
  "users" [list<String>]: Required, list of users/groups to update the policies for.

  "columns"[list<String>]: Optional, list of columns to grant to these users.
          [*] grants all columns.
  "filters"[String] : Optional, SQL where clause. If specified only records passing this
          filter are granted.
}
```
Grants the list of users access to the dataset (by name or id). Cell level permissions
can be controlled by specifying columns/filters which behave identically as creating
new data sets. Returns the id for this policy.

### Revoking policies
Endpoint: /api/policies/revoke POST
```
{
  "policy-id" [long]: id of policy.
  "users" [list<String>]: Users to remove from this policy.
}
```
Removes the list of users from this policy.

### Misc functions

##### Deleting datasets
Endpoint: /api/datasets/{id} delete POST

Deletes the dataset.

##### Retrieving datasets
Endpoint: /api/policies/revoke POST
```
{
  "prefix" [String]: Optional, prefix of datasets to return.
}
```
Returns list of dataset ids.

##### Retrieving dataset
Endpoint: /api/datasets/{id} POST

Returns: Dataset json.

##### Deleting policy
Endpoint: /api/policies/{id}/delete POST

Deletes this policy revoking all access it granted.

##### Retrieving policy
Endpoint: /api/policy/{id} POST

Returns: Policy json.

### Example
Here are a few examples for common tasks using this API.

To register a dataset for every to access, you would:
```
/api/datasets [PUT] { "name": "data", "url": "s3://for-everyone" }
/api/policies/grant [POST] { "dataset": "data", "users" = ["*"] }
```

To register a dataset where two users have access to different columns:
```
/api/datasets [PUT] { "name": "data", "base_url": "s3://for-everyone" }
/api/policies/grant [POST] { "dataset": "data", "users" = ["u1"], "columns" = ["c1", "c2"] }
/api/policies/grant [POST] { "dataset": "data", "users" = ["u2"], "columns" = ["mask(c2)"] }
```

